{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "from typing import Any, Dict, Iterator, List, Optional\n",
    "from langchain_core.callbacks.manager import CallbackManagerForLLMRun\n",
    "from langchain_core.language_models.llms import LLM\n",
    "from langchain_core.outputs import GenerationChunk\n",
    "import json\n",
    "from dify_client import ChatClient\n",
    "from pydantic import Field, HttpUrl\n",
    "\n",
    "class ChatFlowLLM(LLM):\n",
    "    api_key: str = Field(..., description=\"API key for Dify\")\n",
    "    user_id: str = Field(..., description=\"User ID for Dify\")\n",
    "    base_url: HttpUrl = Field(..., description=\"Base URL for Dify API\")\n",
    "    chat_client: ChatClient = Field(None, exclude=True)\n",
    "\n",
    "    def __init__(self, **data):\n",
    "        super().__init__(**data)\n",
    "        self.chat_client = ChatClient(api_key=self.api_key)\n",
    "        self.chat_client.base_url = str(self.base_url)\n",
    "\n",
    "    def _call(self, prompt: str, stop: Optional[List[str]] = None, run_manager: Optional[CallbackManagerForLLMRun] = None, **kwargs: Any) -> str:\n",
    "        if stop is not None:\n",
    "            raise ValueError(\"stop kwargs are not permitted.\")\n",
    "        \n",
    "        chat_response = self.chat_client.create_chat_message(inputs={}, query=prompt, user=self.user_id, response_mode=\"blocking\")\n",
    "        chat_response.raise_for_status()\n",
    "        response_data = chat_response.json()\n",
    "        return response_data.get('answer', '')\n",
    "\n",
    "    def _stream(self, prompt: str, stop: Optional[List[str]] = None, run_manager: Optional[CallbackManagerForLLMRun] = None, **kwargs: Any) -> Iterator[GenerationChunk]:\n",
    "        chat_response = self.chat_client.create_chat_message(inputs={}, query=prompt, user=self.user_id, response_mode=\"streaming\")\n",
    "        chat_response.raise_for_status()\n",
    "\n",
    "        for line in chat_response.iter_lines(decode_unicode=True):\n",
    "            line = line.split('data:', 1)[-1]\n",
    "            if line.strip():\n",
    "                line = json.loads(line.strip())\n",
    "                chunk = GenerationChunk(text=line.get('answer', ''))\n",
    "                if run_manager:\n",
    "                    run_manager.on_llm_new_token(chunk.text, chunk=chunk)\n",
    "                yield chunk\n",
    "\n",
    "    @property\n",
    "    def _identifying_params(self) -> Dict[str, Any]:\n",
    "        return {\"model_name\": \"ChatFlowLLM\"}\n",
    "\n",
    "    @property\n",
    "    def _llm_type(self) -> str:\n",
    "        return \"dify_custom\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "結果: こんにちは！今日はどんなお手伝いができますか？\n",
      "ストリーム結果:\n",
      "こんにちは！どうかしましたか？今日はどんなことをお話ししましょうか？"
     ]
    }
   ],
   "source": [
    "\n",
    "# 使用例\n",
    "api_key = \"app-xxxxxxxxxxxxx\"\n",
    "user_id = \"user_id\"\n",
    "base_url = \"https://example.com\"\n",
    "\n",
    "dify_llm = DifyCustomLLM(api_key=api_key, user_id=user_id, base_url=base_url)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'It sounds like you might be referencing something specific when you mention \"foobar.\" Could you please provide more context or specify what you mean by \"foobar\"? The term \"foobar\" is often used as a placeholder name in computer programming and other technical documentation, similar to \"foo\" and \"bar.\" It might represent a variable, function, or any sort of example element. If you can give more details about the context—whether it\\'s a tech problem, a coding exercise, or something else—I can offer a more targeted response.'"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dify_llm.invoke(\"This is a foobar thing\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
